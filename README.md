# Numerical-Optimisation
Implemented various gradient descent methods from scratch like Vanilla Gradient Descent, Mini-batch, Stochastic, Momentum, NAG, Adagrad, RMSProp, and Adam Optimizers.
